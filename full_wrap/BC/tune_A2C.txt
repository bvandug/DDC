import optuna
import numpy as np
from stable_baselines3 import PPO, SAC, TD3, A2C, DDPG
from stable_baselines3.common.noise import NormalActionNoise
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.evaluation import evaluate_policy
from BCSimTestEnv import BCSimulinkEnv
import json
import os
import torch
from torch import nn
from tqdm import tqdm
import sys
import argparse

# Define the root path for local Colab storage
LOCAL_COLAB_ROOT = "/content/rl_tuning_data"


class OptunaPruningCallback(BaseCallback):
    def __init__(self, trial: optuna.Trial, eval_freq: int, n_eval_episodes: int, verbose: int = 0):
        super().__init__(verbose)
        self.trial = trial
        self.eval_freq = eval_freq
        self.n_eval_episodes = n_eval_episodes
        self.eval_env = None

    def _init_callback(self) -> None:
        eval_env_fn = lambda: BCSimulinkEnv(
            model_name="bcSim",
            enable_plotting=False,
            max_episode_time=0.01
        )
        self.eval_env = DummyVecEnv([eval_env_fn])
        self.eval_env = VecNormalize(self.eval_env, norm_obs=True, norm_reward=False, clip_obs=10.0)

    def _on_step(self) -> bool:
        if self.n_calls % self.eval_freq == 0:
            try:
                mean_reward, _ = evaluate_policy(self.model, self.eval_env,
                                                 n_eval_episodes=self.n_eval_episodes,
                                                 deterministic=True,
                                                 return_episode_rewards=False)
            except Exception as e:
                print(f"Evaluation failed in pruning callback: {e}")
                mean_reward = -np.inf

            self.trial.report(mean_reward, self.n_calls)

            if self.trial.should_prune():
                print(f"Trial {self.trial.number} pruned at step {self.n_calls}.")
                self.eval_env.close()
                raise optuna.exceptions.TrialPruned()
        return True

    def _on_training_end(self) -> None:
        if self.eval_env:
            self.eval_env.close()


def objective(trial, algo_name):
    env_fn = lambda: BCSimulinkEnv(
        model_name="bcSim",
        enable_plotting=False,
        max_episode_time=0.01
    )
    env = DummyVecEnv([env_fn])
    env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=10.0)

    try:
        device = "cuda" if torch.cuda.is_available() else "cpu"

        # --- Policy Network Parameters (Common to all algos using MlpPolicy) ---
        # THESE ARE MOVED HERE TO ENSURE THEY ARE ALWAYS DEFINED
        n_layers = trial.suggest_int("n_layers", 2, 4)
        layer_size = trial.suggest_int("layer_size", 64, 256)
        activation_fn = {"tanh": nn.Tanh, "relu": nn.ReLU}[trial.suggest_categorical("activation_fn", ["tanh", "relu"])]
        net_arch = [layer_size] * n_layers
        policy_kwargs = {"net_arch": net_arch, "activation_fn": activation_fn}
        # --- End Policy Network Parameters ---

        if algo_name.lower() == "td3":
            params = {
                "learning_rate": trial.suggest_float("learning_rate", 1e-5, 1e-3, log=True),
                "buffer_size": trial.suggest_int("buffer_size", 20000, 100000),
                "batch_size": trial.suggest_categorical("batch_size", [64, 128, 256, 512]),
                "tau": trial.suggest_float("tau", 0.005, 0.02),
                "gamma": trial.suggest_float("gamma", 0.95, 0.999),
                "action_noise_sigma": trial.suggest_float("action_noise_sigma", 0.05, 0.2),
                # n_layers, layer_size, activation_fn now come from above
            }
            # net_arch and activation_fn were previously defined inside this block
            # They are now defined once at the top of objective function
            # net_arch = [params["layer_size"]] * params["n_layers"] # REMOVED
            # activation_fn = {"tanh": nn.Tanh, "relu": nn.ReLU}[params["activation_fn"]] # REMOVED
            # policy_kwargs = {"net_arch": net_arch, "activation_fn": activation_fn} # REMOVED

            model = TD3(
                "MlpPolicy", env, verbose=0, device=device,
                learning_rate=params["learning_rate"],
                buffer_size=params["buffer_size"],
                batch_size=params["batch_size"],
                tau=params["tau"],
                gamma=params["gamma"],
                action_noise=NormalActionNoise(mean=np.zeros(1), sigma=params["action_noise_sigma"] * np.ones(1)),
                policy_kwargs=policy_kwargs, # Uses the policy_kwargs defined at the top
            )

        elif algo_name.lower() == "sac":
            params = {
                "learning_rate": trial.suggest_float("learning_rate", 1e-5, 1e-3, log=True),
                "buffer_size": trial.suggest_int("buffer_size", 20000, 100000),
                "batch_size": trial.suggest_categorical("batch_size", [64, 128, 256, 512]),
                "gamma": trial.suggest_float("gamma", 0.95, 0.999),
                "tau": trial.suggest_float("tau", 0.005, 0.02),
                "ent_coef": trial.suggest_float("ent_coef", 1e-8, 0.1, log=True),
                "learning_starts": trial.suggest_int("learning_starts", 100, 1000),
                "train_freq": trial.suggest_categorical("train_freq", [1, 4, 8, 16]),
                "target_update_interval": trial.suggest_int("target_update_interval", 1, 10),
            }
            # net_arch and activation_fn were previously defined inside this block
            # They are now defined once at the top of objective function
            # net_arch = [params["layer_size"]] * params["n_layers"] # REMOVED
            # activation_fn = {"tanh": nn.Tanh, "relu": nn.ReLU}[params["activation_fn"]] # REMOVED
            # policy_kwargs = {"net_arch": net_arch, "activation_fn": activation_fn} # REMOVED

            model = SAC(
                "MlpPolicy", env, verbose=0, device=device,
                learning_rate=params["learning_rate"],
                buffer_size=params["buffer_size"],
                batch_size=params["batch_size"],
                gamma=params["gamma"],
                tau=params["tau"],
                ent_coef=params["ent_coef"],
                learning_starts=params["learning_starts"],
                train_freq=(params["train_freq"], "step"),
                target_update_interval=params["target_update_interval"],
                policy_kwargs=policy_kwargs, # Uses the policy_kwargs defined at the top
            )

        elif algo_name.lower() == "a2c":
            params = {
                "learning_rate": trial.suggest_float("learning_rate", 1e-5, 1e-3, log=True),
                "gamma": trial.suggest_float("gamma", 0.90, 0.999),
                "n_steps": trial.suggest_int("n_steps", 8, 2048, log=True),
                "ent_coef": trial.suggest_float("ent_coef", 1e-8, 0.01, log=True),
                "vf_coef": trial.suggest_float("vf_coef", 0.2, 0.8),
                "max_grad_norm": trial.suggest_float("max_grad_norm", 0.3, 5.0),
                # n_layers, layer_size, activation_fn now come from above
            }
            # net_arch and activation_fn were previously defined inside this block
            # They are now defined once at the top of objective function
            # net_arch = [params["layer_size"]] * params["n_layers"] # REMOVED
            # activation_fn = {"tanh": nn.Tanh, "relu": nn.ReLU}[params["activation_fn"]] # REMOVED
            # policy_kwargs = {"net_arch": net_arch, "activation_fn": activation_fn} # REMOVED

            model = A2C(
                "MlpPolicy", env, verbose=0, device=device,
                learning_rate=params["learning_rate"],
                gamma=params["gamma"],
                n_steps=params["n_steps"],
                ent_coef=params["ent_coef"],
                vf_coef=params["vf_coef"],
                max_grad_norm=params["max_grad_norm"],
                policy_kwargs=policy_kwargs, # Uses the policy_kwargs defined at the top
            )

        else:
            raise ValueError(f"Unsupported algorithm: {algo_name}")

        model.learn(total_timesteps=10000, callback=OptunaPruningCallback(trial, eval_freq=1000, n_eval_episodes=1), progress_bar=True)

        mean_reward = 0
        n_eval_episodes = 1
        for _ in range(n_eval_episodes):
            obs, _ = env.reset()
            done = False
            episode_reward = 0
            while not done:
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, _ = env.step(action)
                done = terminated or truncated
                episode_reward += reward
            mean_reward += episode_reward

        return mean_reward / n_eval_episodes

    finally:
        env.close()


def tune_hyperparameters(algo_name, n_trials):
    study_name = f"{algo_name}-bc-tuning"
    study_db_path = os.path.join(LOCAL_COLAB_ROOT, "tuning_studies", f"{study_name}.db")
    os.makedirs(os.path.dirname(study_db_path), exist_ok=True)
    storage_name = f"sqlite:///{study_db_path}"

    print(f"\nTuning {algo_name.upper()}. Study: {study_name}")
    print(f"Results will be saved to: {storage_name}")
    print("WARNING: Data stored locally is TEMPORARY and will be lost on session disconnect/restart.")

    study = optuna.create_study(
        study_name=study_name,
        storage=storage_name,
        load_if_exists=True,
        direction="maximize",
        pruner=optuna.pruners.MedianPruner(
            n_startup_trials=5,
            n_warmup_steps=0,
            interval_steps=1000
        )
    )

    for i in tqdm(range(n_trials), desc=f"Tuning {algo_name.upper()}"):
        print(f"\nâ–¶ Running trial {i + 1}/{n_trials}")
        study.optimize(lambda trial: objective(trial, algo_name), n_trials=1, n_jobs=1)


    best_params = study.best_params
    best_value = study.best_value

    results = {
        "best_value": best_value,
        "best_params": best_params,
    }

    results_dir = os.path.join(LOCAL_COLAB_ROOT, "hyperparameter_results")
    os.makedirs(results_dir, exist_ok=True)
    results_file_path = os.path.join(results_dir, f"{algo_name}_best_params.json")

    with open(results_file_path, "w") as f:
        json.dump(results, f, indent=4)

    print(f"\n--- Best parameters for {algo_name} ---")
    print(f"Best value (mean reward): {best_value:.4f}")
    for key, value in best_params.items():
        print(f"  {key}: {value}")

    return best_params


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run hyperparameter tuning for RL algorithms on Buck Converter.")
    parser.add_argument("--algo", type=str, default="a2c", choices=["a2c", "td3", "sac"],
                        help="RL algorithm to tune (a2c, td3, or sac).")
    parser.add_argument("--n_trials", type=int, default=20,
                        help="Number of Optuna trials to run.")
    args = parser.parse_args()

    print(f"{'=' * 50}")
    print(f"Tuning hyperparameters for {args.algo.upper()}...")
    print(f"{'=' * 50}\n")

    tune_hyperparameters(
        algo_name=args.algo,
        n_trials=args.n_trials
    )